An example ETL pipeline using PySpark that reads data from a JSON file, applies some data transformations, and writes the transformed data to a MySQL database.
hashtag#pypsark code 

# Import necessary modules
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws, year, month, dayofmonth
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Create a SparkSession
spark = SparkSession.builder.appName("ETL Pipeline").getOrCreate()

# Define schema for JSON data
schema = StructType([
 StructField("customer_id", IntegerType(), True),
 StructField("order_id", IntegerType(), True),
 StructField("order_date", StringType(), True),
 StructField("order_amount", StringType(), True),
 StructField("product_name", StringType(), True)
])

# Read JSON data into PySpark DataFrame
df = spark.read.json("input.json", schema=schema)

# Apply data transformations
df_transformed = df.select(
 col("customer_id"),
 year(col("order_date")).alias("order_year"),
 month(col("order_date")).alias("order_month"),
 dayofmonth(col("order_date")).alias("order_day"),
 col("order_amount").cast("float").alias("order_amount"),
 concat_ws("-", col("customer_id"), col("order_id")).alias("order_key"),
 col("product_name")
)

# Write transformed data to MySQL database
url = "jdbc:mysql://localhost:3306/mydatabase"
table_name = "orders"
mode = "append"
properties = {
 "driver": "com.mysql.cj.jdbc.Driver",
 "user": "username",
 "password": "password"
}

df_transformed.write.jdbc(url=url, table=table_name, mode=mode, properties=properties)

In this example, we create a SparkSession and read data from a JSON file using the spark.read method. We also define a schema for the JSON data using PySpark's StructType and StructField classes. We then apply some data transformations using PySpark's col, year, month, dayofmonth, and concat_ws functions. Finally, we write the transformed data to a MySQL database using PySpark's write.jdbc method.
Note that you'll need to replace the MySQL database credentials (url, table_name, properties) with your own information in order to write to your own database.
