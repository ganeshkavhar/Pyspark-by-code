from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, avg, max, min, count

# Initialize Spark session
spark = SparkSession.builder.appName("AggExample").getOrCreate()

# Sample data: (department, employee, salary)
data = [
    ("HR", "Alice", 3000),
    ("HR", "Bob", 4000),
    ("IT", "Charlie", 5000),
    ("IT", "David", 6000),
    ("Finance", "Edward", 7000)
]

# Define schema
columns = ["department", "employee", "salary"]

# Create DataFrame
df = spark.createDataFrame(data, schema=columns)

# Show the initial DataFrame
print("Initial DataFrame:")
df.show()

# Group by department and apply aggregate functions
agg_df = df.groupBy("department").agg(
    sum("salary").alias("total_salary"),
    avg("salary").alias("average_salary"),
    max("salary").alias("max_salary"),
    min("salary").alias("min_salary"),
    count("employee").alias("num_employees")
)

# Show the resulting DataFrame
print("Aggregated DataFrame:")
agg_df.show()
