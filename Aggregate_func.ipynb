from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col, count, sum, avg, min, max, first, last

# Initialize Spark session
spark = SparkSession.builder.appName("WindowFunctionsExample").getOrCreate()

# Sample data
data = [
    ("Alice", "HR", 3000),
    ("Bob", "Engineering", 4000),
    ("Charlie", "Engineering", 4500),
    ("David", "HR", 3500),
    ("Eve", "Engineering", 5000),
    ("Frank", "HR", 4000)
]

# Define schema
columns = ["name", "department", "salary"]

# Create DataFrame
df = spark.createDataFrame(data, schema=columns)

# Define window specification
windowSpec = Window.partitionBy("department").orderBy("salary")

# Apply window functions
df_with_window_funcs = df \
    .withColumn("row_count", count("*").over(windowSpec)) \
    .withColumn("sum_salary", sum("salary").over(windowSpec)) \
    .withColumn("avg_salary", avg("salary").over(windowSpec)) \
    .withColumn("min_salary", min("salary").over(windowSpec)) \
    .withColumn("max_salary", max("salary").over(windowSpec)) \
    .withColumn("first_salary", first("salary").over(windowSpec)) \
    .withColumn("last_salary", last("salary").over(windowSpec))

# Show the result
df_with_window_funcs.show()
