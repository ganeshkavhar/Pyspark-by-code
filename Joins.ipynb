from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("JoinExample").getOrCreate()

# Sample data for DataFrame 1 (students)
data1 = [
    (1, "Alice", 20),
    (2, "Bob", 22),
    (3, "Charlie", 23)
]

# Define schema for DataFrame 1
columns1 = ["id", "name", "age"]

# Create DataFrame 1
df1 = spark.createDataFrame(data1, schema=columns1)

# Sample data for DataFrame 2 (subjects)
data2 = [
    (1, "Math"),
    (2, "Science"),
    (4, "History")
]

# Define schema for DataFrame 2
columns2 = ["id", "subject"]

# Create DataFrame 2
df2 = spark.createDataFrame(data2, schema=columns2)

# Show DataFrames
print("DataFrame 1:")
df1.show()

print("DataFrame 2:")
df2.show()

# Inner join
inner_join_df = df1.join(df2, df1.id == df2.id, "inner")
print("Inner Join:")
inner_join_df.show()

# Left join
left_join_df = df1.join(df2, df1.id == df2.id, "left")
print("Left Join:")
left_join_df.show()

# Right join
right_join_df = df1.join(df2, df1.id == df2.id, "right")
print("Right Join:")
right_join_df.show()

# Full outer join
full_outer_join_df = df1.join(df2, df1.id == df2.id, "outer")
print("Full Outer Join:")
full_outer_join_df.show()
